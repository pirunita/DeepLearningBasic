{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 perceptron to Neural Network ##\n",
    "perceptron과 neural network의 차이점은 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 신경망의 예 ###\n",
    "input layer - hidden layer - output layer으로 구성, 가중치를 갖는 층은 2개뿐이므로 2층 신경망이라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 퍼셉트론 복습 ###\n",
    "x<sub>1</sub>, x<sub>2</sub>라는 두 신호를 입력받아 y를 출력하는 퍼셉트론의 경우는 다음과 같다.<br>\n",
    "이 때 조건 분기의 동작(0을 넘으면 1을 출력, 그렇지 않으면 0)\n",
    "\n",
    "y = 0 (b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> <= 0)<br>\n",
    "y = 1 (b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> > 0)<br>\n",
    "이 때 b는 bias를 나타내는 매개변수, 뉴런이 얼마나 쉽게 활성화되는지 제어 한다.<br>\n",
    "그림에서 b는 input = 1, weight = b일 때와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식을 더 간결하게 만들면, 조건 분기의 동작을 하나의 함수로 나타낸다.<br>\n",
    "y = h(b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>)<br>\n",
    "h(x) = 0(x <= 0)<br>\n",
    "h(x) = 1(x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Activation function ###\n",
    "위에서 h(x)처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 activation function라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = h(b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>)<br>\n",
    "이를 다시 써보면 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 activation function에 입력하여 결과를 내는 2단계로 처리한다.<br>\n",
    "a = b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>, weight가 달린 입력 신호와 bias의 총합을 계산<br>\n",
    "y = h(a), 위의 a를 함수 h()에 넣어 y를 출력하는 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Activation Function ##\n",
    "위에서 본 h(x)와 같은 활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이를 step function이라 한다. 퍼셉트론은 활성화 함수로 계단 함수를 이용한다고 할 수 있다. 활성화 함수를 계단에서 다른 함수로 변경하는 것이 신경망을 배우는 길이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 sigmoid function ###\n",
    "신경망에서 자주 이용하는 활성화 함수는 **sigmoid function**이다.<br>\n",
    "h(x) = 1/(1 + exp(-x))<br>\n",
    "신경망에서 활성화 함수로 sigmoid function를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. 앞에서 본 perceptron과 신경망의 주된 차이는 활성화 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 step function 구현하기 ###\n",
    "간단하다, 입력이 0을 넘으면 1을 출력, 그 외에는 0을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 위 식의 x는 float만 받아드린다. 따라서 input으로 넘파이 배열을 위해 다음과 같이 구현 해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)\n",
    "# astype() : 넘파이 배열의 자료형을 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.  2.]\n",
      "[False  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "print(x)\n",
    "y = x > 0\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 원하는 step function은 0이나 1의 int형을 출력해야 함\n",
    "y = y.astype(np.int)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Step function's graph ###\n",
    "matplotlib를 통해 step function을 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.2)\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 sigmoid function's graph ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.arange(-5.0, 5.0, 1.0)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Step vs Sigmoid ###\n",
    "1. Sigmoid는 곡선형태의 연속함수를 가져 연속적인 실수를 갖는다. 이는 신경망에서 연속적인 float가 들어가고 나온다.\n",
    "2. 두 함수 모두 작으면 0에 가깝고 크면 1에 가까워진다.\n",
    "3. 두 함수 모두 비선형 함수, 즉 직선 1개로 그릴 수 없는 함수다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에서 activation function으로 **non-linear function**을 사용해야 한다. linear function은 신경망 층을 깊게 해도 계속 선형함수가 되므로 hidden layer가 없는 Network와 다를 바 없다. step function과 sigmoid function 모두 non-linear function이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 ReLU function ###\n",
    "최근에 자주 사용, input이 0을 넘으면 그대로 출력, 0 이하면 0을 출력<br>\n",
    "h(x) = x (x > 0)<br>\n",
    "h(x) = 0 (x <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "# maximum : 두 입력 중 큰 값을 선택해 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 다차원 배열의 계산 ##\n",
    "다차원 배열을 사용하여 계산법을 익히고 신경망을 구현하여 보자.<br>\n",
    "np.ndim() : 배열의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "2\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(B)\n",
    "print(np.ndim(B))\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 행렬의 곱 ###\n",
    "행렬의 곱셈은 np.matmul()을 통해 element wise multiple 수행, 만약 두 행렬이 2차원이라면 np.dot 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2)\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2], [3,4]])\n",
    "B = np.array([[5,6], [7,8]])\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "print(np.dot(A, B))\n",
    "print(np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix의 shape를 잘 맞춰 대응하는 차원의 element 수를 일치시켜야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 신경망에서의 행렬곱 ###\n",
    "대응하는 차원의 element 수가 반드시 같아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2])\n",
    "print(X.shape)\n",
    "W = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "print(W.shape)\n",
    "Y = np.matmul(X, W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 3층 신경망 구현 ##\n",
    "input layer(0층) 2개<br>\n",
    "hidden layer1(1층) 3개<br>\n",
    "hidden layer2(2층) 2개<br>\n",
    "output layer(3층) 2개<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x<sub>1</sub>, x<sub>2</sub>]<br>\n",
    "[a<sub>1</sub><sup>(1)</sup>, a<sub>2</sub><sup>(1)</sup>, a<sub>3</sub><sup>(1)</sup>]<br>\n",
    "...<br>\n",
    "[y<sub>1</sub>, y<sub>2</sub>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 1층의 weight part를 간소화하면 다음과 같다.<br>\n",
    "A<sup>(1)</sup> = XW<sup>(1)</sup> + B<sup>(1)</sup><br>\n",
    "A<sup>(1)</sup> = (a<sub>1</sub><sup>(1)</sup> a<sub>2</sub><sup>(1)</sup> a<sub>3</sub><sup>(1)</sup>)<br>\n",
    "X = (x<sub>1</sub> x<sub>2</sub>)<br>\n",
    "B<sup>(1)</sup> = (b<sub>1</sub><sup>(1)</sup> b<sub>2</sub><sup>(1)</sup> b<sub>3</sub><sup>(1)</sup>)<br>\n",
    "W<sup>(1)</sup> = (w<sub>11</sub><sup>(1)</sup> w<sub>21</sub><sup>(1)</sup> w<sub>31</sub><sup>(1)</sup>)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w<sub>12</sub><sup>(1)</sup> w<sub>22</sub><sup>(1)</sup> w<sub>32</sub><sup>(1)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n",
      "(3,)\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "A1 = np.matmul(X, W1) + B1\n",
    "Z1 = sigmoid(A1)\n",
    "print(A1)\n",
    "print(A1.shape)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(2,)\n",
      "[0.62624937 0.7710107 ]\n"
     ]
    }
   ],
   "source": [
    "# 1층에서 2층으로 가는 구현\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.matmul(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identity function을 output layer의 actiation function으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def identity_function(x):\n",
    "    return x\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "A3 = np.matmul(Z2, W3) + B3\n",
    "Y = identity_function(A3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 출력층 설계 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output layer's activation function은 문제의 성질에 맞게 정한다<br>\n",
    "회귀 : identity function<br>\n",
    "2클래스 분류 : sigmoid function<br>\n",
    "다중클래스 분류 : softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 identity function & softmax function 구현 ###\n",
    "분류에서 사용, 분자 : a<sub>k</sub>의 지수함수, 분모는 모든 입력 신호의 지수함수의 합<br>\n",
    "softmax의 output은 모든 input signal로부터 화살표를 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n",
      "74.1221542101633\n",
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a)\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "print(exp_a)\n",
    "print(sum_exp_a)\n",
    "\n",
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 softmax function 구현 시 주의 사항 ###\n",
    "지수함수 e는 큰 값을 쉽게 만들 수 있기 때문에 overflow가 생길 수 있다.<br>\n",
    "따라서 overflow를 막기 위해 입력 신호 중 최대값을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosungpil/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 -10 -20]\n",
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n"
     ]
    }
   ],
   "source": [
    "c = np.max(a)\n",
    "print(a - c)\n",
    "print(softmax(a - c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overflow를 막은 softmax function 재구현\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 softmax function 특징 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax(a)\n",
    "print(y)\n",
    "print(np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function의 output은 0에서 1.0 사이의 실수이며 output의 총합이 1이 된다는 점이 중요한 성질이므로 각 출력을 확률로 해석할 수 있다 따라서 softmax function을 이용해 문제를 확률적으로 대응하여 분류 문제를 해결할 수 있을 것이다..<br>\n",
    "또한 softmax function을 통해 각 element의 대소관계는 변하지 않는다. 이는 지수함수가 단조 증가이기 때문<br>\n",
    "따라서 softmax function은 출력이 가장 큰 뉴런의 위치는 변하지 않기 때문에 사실상 함수를 생략해도 된다. training, inference에서 <u>inference에서는 생략</u>하는 것이 일반적이고(계산으로 인한 자원 낭비), <u>training시에는 softmax function을 사용</u>한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 output layer의 neuron 수 정하기 ###\n",
    "분류 : 분류하고 싶은 클래스 수로 설정하는 것이 일반적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 손글씨 숫자 인식 ##\n",
    "#### 저장이 안 돼서 정리한 게 날아갔으니 코드만 씀 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9352\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "def get_data():\n",
    "        (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=True, one_hot_label=False)\n",
    "        return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pkl.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.matmul(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# 배치로 구현\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i : i + batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis = 1)\n",
    "    accuracy_cnt += np.sum(p == t[i : i + batch_size])\n",
    "    \n",
    "print(\"Accuracy: \" + str(float(accuracy_cnt) / len(x)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
