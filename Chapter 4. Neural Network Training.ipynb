{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training : training data로부터 weight parameter의 최적값을 자동으로 획득하는 것이다.<br>\n",
    "이 때 Neural Network가 학습할 수 있도록 하는 함수가 **loss function**이며, loss function의 결과를 가장 작게 만드는 weight function을 찾는 것이 **training의 목적**이 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 데이터로부터 학습 ##\n",
    "Neural network는 데이터를 보고 weight parameter을 자동으로 결정한다는 특징이 있다. 이 때 parameter은 엄청나게 늘어날 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 데이터 주도 학습 ###\n",
    "Machine learning에서는 Data로부터 패턴을 찾으려고 한다. 주어진 Data로부터 어떤 문제를 해결하기 위해서는 <u>이미지에서 feature을 추출하고 그 feature의 pattern을 training</u>하는 방법이 있다.<br>\n",
    "* Feature : Input data에서 원하는 data를 정확하게 추출할 수 있도록 설계된 transformer, 특히 Image feature은 vector로 기술한다.\n",
    "* Image feature에는 SIFT, SURF, HOG 등의 특징을 이용\n",
    "* 이미지로부터 변환된 vector로 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습을 진행\n",
    "\n",
    "Machine learning은 사람의 개입을 최소화하지만 neural network와 Deep learning을 통해 사람의 개입을 더욱 배제할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network는 이미지를 그대로 학습한다. 이는 Neural network가 이미지에 포함된 중요한 feature도 스스로 학습하여 사람의 개입이 없게 될 것이다. 따라서 end-to-end machine learning이라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Training data & Test data ###\n",
    "Training data만으로 학습하여 최적의 parameter를 찾고 Test data를 사용하여 training model을 평가한다. 모델은 범용적으로 사용되어야 하므로 새로운 data에도 문제를 잘 풀어내야 하며 이것이 최종 목표다.<br>\n",
    "Dataset 하나만으로 parameter의 학습과 평가를 한다면 범용성이 떨어질 수 있다. <u>한 dataset에만 지나치게 최적화된 상태를 <b>overfitting</b>이라고 한다.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 loss function ##\n",
    "Neural network에서도 현재의 상태를 **하나의 지표**로 나타내고, 그 지표를 최적화하기 위해선 weight parameter의 값을 탐색한다. 이 때 그 지표를 **loss function**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Mean Squared Error, MSE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\frac{1}{2} \\sum_{k}(y_k - t_k)^2$$\n",
    "* y<sub>k</sub> : 신경망 출력으로 softmax function output -> 추정값으로 확률로 해석\n",
    "* t<sub>k</sub> : 정답 label, 정답을 가리키는 element는 1, 그 외에는 0 -> **One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # One-hot vector, 정답 2\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # softmax function output 임의로 설정, 2일 확률이 가장 높음\n",
    "print(mean_squared_error(np.array(y1), np.array(t)))\n",
    "\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 일부러 틀린 one-hot vector 도입 = 신경망 출력이 7일 때를 가정\n",
    "print(mean_squared_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째가 loss function의 output이 작으며 정답 label과도 오차가 적다. MSE로 봤을 때 첫 번째가 정답에 더 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Cross Entropy Error, CEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = - \\sum_{k}t_k\\log y_k$$\n",
    "* log : 밑이 e인 자연로그 (ln)\n",
    "* y<sub>k</sub> : 신경망 output\n",
    "* t<sub>k</sub> : 정답 label\n",
    "<br>\n",
    "t<sub>k</sub>가 1일 때의 추정값 y<sub>k</sub>의 자연로그를 계산하는 식이 된다. 이는 정답일 때의 출력이 전체 값을 정하게 된다.<br>\n",
    "* Ex) 정답 label의 output이 0.6이라면 -log0.6 = 0.51, 만약 output이 0.1이라면 -log0.1 = 2.30\n",
    "따라서 CEE는 <u>정답에 해당하는 output이 커질수록 0에 다가가고</u> 출력이 1이면 0이된다. 반대의 정답일 경우 출력이 작아져 오차가 커지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] \n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] \n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 값 delta를 더함으로써 -inf를 발생하지 않도록 작은 값을 더해 0이 되지 않게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Mini-batch Training ###\n",
    "이제는 데이터 모두에 대한 loss function의 합을 구하는 방법을 생각한다. cross entropy error는 다음과 같이 나타낼 수 있다.\n",
    "$$E = -\\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}$$\n",
    "* 위에 나타낸 data 하나에 대한 loss function을 N개의 데이터로 확장하고 N으로 나누어 normalization, 즉 '평균 손실 함수'를 구하여 <u>training data 개수와 관계없이 통일된 지표를 얻을 수 있다.</u>\n",
    "* Data 수가 많아지면 데이터를 일일이 loss function을 계산하는 것은 시간이 걸리므로 데이터 일부를 추려 전체의 **근사치**로 이용\n",
    "* 학습 시 training data로부터 일부만 골라 학습을 수행하는 **mini-batch training** 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[21116 34510 59956 26310    89 53164 55133 15923 50666 36865]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "# training data : 60,000\n",
    "# input data : 784(28 * 28)\n",
    "# 정답 label : 10줄\n",
    "\n",
    "train_size = x_train.shape[0] # 60,000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 60,000개의 training data에서 10장만 추출\n",
    "print(batch_mask)\n",
    "\n",
    "# mini-batch 구성\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) Cross Entropy Error 구현 ###\n",
    "데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우를 모두 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y : 신경망 output\n",
    "* t : 정답 label\n",
    "* y가 1차원이라면 데이터 하나당 CEE를 구하는 경우 reshape로 data의 shape를 바꾼 후 batch_size로 나눠 정규화하여 **이미지 1장 당 평균의 cross entropy error**를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정답 label이 one-hot vector가 아닌 '2', '7' 등의 숫자 label로 주어졌을 때 CEE를 구하는 방법이다.\n",
    "* 핵심은 t가 0인 element는 CEE도 0이므로 그 계산은 skip하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log(y[np.arange(batch_size), t])\n",
    "* np.arange(batch_size)는 0부터 batch_size - 1까지 배열 생성, batch_size가 5라면 [0, 1, 2, 3, 4] 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 loss function을 설정하는 이유 ###\n",
    "* Neural network에서 최적의 parameter(weight, bias)를 탐색할 때 loss function의 값을 최소화 하는 parameter값을 찾는다.\n",
    "* parameter의 미분을 통해 서서리 갱신\n",
    "* **weight parameter의 loss function 미분 ** 은 <u>weight parameter를 아주 미세하게 변화시켰을 때 어떻게 loss function이 변하는지?</u>\n",
    "* 미분 값이 0이면 weight parameter는 어디로 움직여도 loss function 값이 달라지지 않으므로 accuracy를 지표로 삼지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
